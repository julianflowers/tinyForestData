------------------------------------------------------------------------
---

title: "TLDR" 
author: "Julian Flowers" 
editor: visual date: "`r Sys.Date()`" 
execute: 
    echo: false 
    warning: false 
    message: false

---

```{r}

remotes::install_github("https://github.com/KTH-Library/semanticscholar")
library(semanticscholar)
library(tidyverse)
library(furrr)
install.packages("spacyr")
library(spacyr)
library(reticulate)
library(reactable)
library(reactablefmtr)
library(ggrepel)

#virtualenv_create("spacy_venv")
#Sys.setenv(RETICULATE_PYTHON = "/Users/julianflowers/.virtualenvs/spacy_virtualenv/bin/python")

#use_virtualenv("spacy_venv")
# py_install("spacy", pip=TRUE, envname = "spacy_virtualenv")

#spacy_install_virtualenv(python_path = "/Users/julianflowers/.virtualenvs/spacy_venv/bin/python")

spacy_initialize(virtualenv = "spacy_virtualenv")
Sys.getenv("SEMANTICSCHOLAR_API")

semanticscholar::S2_api()


```

```{r}

#S2_fields()

search <- "(urban forest) AND (species richness OR biodiversity*)"

offset <- seq(0, 7000, 100)

sss <- semanticscholar::S2_search_papers(search, limit = 100, fields = c("year,abstract,title,venue,referenceCount,citationCount,fieldsOfStudy"))

sss$total

sss_full <- map_dfr(1:ceiling(sss$total/100), \(x) semanticscholar::S2_search_papers(search, limit = 100, fields = c("year,abstract,title,venue,externalIds,referenceCount,citationCount,fieldsOfStudy"), offset = offset[x]), .progress = TRUE)

sss_full <- sss_full$data |>
  arrange(-year) |>
  unnest_wider("externalIds")


```

Search for `r search` produces `r sss$total` results.

```{r}

sss_full |>
  count(year) |>
  ggplot() +
  geom_col(aes(year, n)) +
  ggtitle(paste0("Searches for ", search))

```


```{r paper-info}

inf <- map(1:sss_full$total, \(x) S2_paper(identifier = sss_full$data$paperId[x]), .progress = TRUE)

doi <- map(inf, "doi") |>
  enframe() 

sss <- sss_full$data |>
  bind_cols(doi)

topics <- map(inf, "topics") |>
  enframe()

sss <- sss |>
  bind_cols(topics)

sss <- sss |>
  unnest("value...11") 

sss |>
  reactable::reactable(showPageSizeOptions = TRUE, sortable = TRUE, filterable = TRUE, defaultPageSize = 50, searchable = TRUE, selection = "multiple") |>
  reactablefmtr::save_reactable_test("urban_gi_biodiversity.html")

```
## Extract tl;dr

```{r tldr}
library(furrr)
plan(multisession)

tldrs <- future_map(1:nrow(sss_full), \(x) S2_paper2(identifier = sss_full$paperId[x], fields="tldr")$tldr$text, .progress = TRUE)

tldrs <- tldrs |>
  enframe() |>
  bind_cols(sss_full) |>
  select(paperId, title, tldr = value, everything()) |>
  arrange(-year)

tldr <- tldrs |>
  unnest_wider("fieldsOfStudy", names_sep = "_") 


```

## Cycle 1

Look at titles and tldrs (where they exist)

```{r}

tldrs |>
  select(paperId, title, tldr, year, fieldsOfStudy) |>
  filter(tldr != "NULL") |>
  unnest("tldr") |>
  reactable::reactable(showPageSizeOptions = TRUE, sortable = TRUE, filterable = TRUE, defaultPageSize = 50, searchable = TRUE, selection = "multiple") |>
  reactablefmtr::save_reactable_test("tldrs1.html")
  


```

```{r ner-abstracts, eval=FALSE}

data_with_abstracts <- tldr |>
  filter(!is.na(abstract)) |>
  mutate(doc_id = paste0("text", row_number()))
# 
anno <- spacy_parse(data_with_abstracts$abstract, entity = TRUE, multithread = 12, nounphrase = TRUE)
# 
np <- nounphrase_extract(anno)
ent <- entity_extract(anno)
# 
locs <- ent |>
   filter(entity_type %in% c("LOC", "GPE"))
# 
data_with_abstracts |>
  left_join(np) |>
  group_by(doc_id) |>
  mutate(nounphrase = paste(nounphrase, collapse = "; ")) |>
  select(-sentence_id) |>
  distinct() |>
  left_join(locs) |>
  mutate(locs = paste(entity, collapse = "; ")) |>
  select(-c(sentence_id, entity_type, entity)) |>
  distinct() |>
  select(paperId, title, nounphrase, locs, year, venue, contains("Count") ) |>
  reactable::reactable(showPageSizeOptions = TRUE, sortable = TRUE, filterable = TRUE, defaultPageSize = 50, searchable = TRUE, selection = "multiple") |>
  reactablefmtr::save_reactable_test("spacy_parsed_abstracts.html")
  

```

## Clustering

```{r}

library(myScrapers)

corp <- myScrapers::create_abstract_corpus(data_with_abstracts |>
                                     rename(absText = abstract, 
                                            pmid = DOI) )

cluster <- create_abstract_cluster(corp$corpus, minPts = 4)

cluster$cluster_size

labels <- create_cluster_labels(corp$corpus, clustering = cluster$clustering)

clus_plot <- labels$results |>
  left_join(data_with_abstracts, by = c("pmid.value" = "DOI")) |>
  select(title, abstract, X1, X2, cluster, doi = pmid.value, clus_names) |>
  ggplot() +
  geom_point(aes(X1, X2, colour = clus_names, label = paste(title, doi )), show.legend = FALSE)+
  geom_point(data = labels$plot, aes(medX, medY, colour = clus_names), shape = "X", size = 5, show.legend = FALSE)

clus_plot +
  stat_ellipse(aes(X1, X2, colour = clus_names), show.legend = FALSE) +
  geom_text_repel(data = labels$plot, aes(medX, medY, label = clus_names, colour = clus_names), show.legend = FALSE, direction = "y") +
  theme_minimal()

library(plotly)

ggplotly(clus_plot)

```



```{r flair, eval=FALSE, results='hide'}

#virtualenv_list()
py_install("flair", pip = TRUE, envname = "spacy_virtualenv")
flair <- import("flair")
ontonotes <- flair$nn$Classifier$load("ner")
bioner <- flair$nn$Classifier$load('bioner')

Sentence <- flair$data$Sentence
splitter <- flair$splitter$SegtokSentenceSplitter

splitter <- splitter()



```

```{r test-abstract, eval=FALSE}


test <- Sentence(data_with_abstracts$abstract[[13]])
test
ontonotes$predict(test)
bioner$predict(test)


test$annotation_layers


```

```{r annotation-function, eval=FALSE}

flair_annotate <- function(text){
  
  txt <- Sentence(text)
  
  ontonotes$predict(txt)
  bioner$predict(txt)
  
  annox <- txt$annotation_layers
  
  return(annox)

}

flair_annotate(data_with_abstracts$abstract[[1]])

safe_annotate <- safely(flair_annotate, otherwise = NA_real_)

annotations <- map(1:nrow(data_with_abstracts), \(x) safe_annotate(data_with_abstracts$abstract[[x]]), .progress = TRUE)

annotations |>
  map("result") |>
  enframe()
  
data_with_abstracts |>
  unnest_wider("externalIds") |>
  reactable::reactable(showPageSizeOptions = TRUE, sortable = TRUE, filterable = TRUE, defaultPageSize = 50, searchable = TRUE, selection = "multiple") |>
  reactablefmtr::save_reactable_test("gi_biodiversity.html")

```


## Format for Zotero upload

Makes use of the `pyzotero` Python package. This needs a Zotero user id and an API key.

```{r}
## install zoteror package
## 

# remotes::install_github("giocomai/zoteror")
# library(zoteror)
# 
# credentials <- zot_auth()
# zot_set_options(user = 8324843, credentials = credentials)
# key <- zot_create_collection(collection_name = "green_inf") 

## install 
reticulate::py_install("pyzotero", pip = TRUE, envname = "spacy_virtualenv")
pyz <- import("pyzotero")
zotero <- pyz$zotero

zoteror::zot_auth()

zoteror::zot_get_item_template(item_type = "journalArticle")
zoteror::zot_set_options(user = 8324843)
zoteror::zot_add_to_collection(my_refs_1$URL, "IZ58FGBA")

## connect
zot <- zotero$Zotero(8324843, library_type = "user", api_key = "nwYkdqxckSNkc7pVWhHLLk5B")

items$top(limit = 4)

## view collections
collections <- zot$collections() |>
  enframe() |>
  unnest_auto("value") |>
  unnest_auto("meta") |>
  unnest_auto("data") |>
  unnest_auto("links")

zot$create_items(my_refs[1:10])


zot$item_types()

zot$item_template(itemtype = "journalArticle")
my_refs <- zotero_references(ids)
my_refs_1 <- my_refs |>
  enframe() |>
  unnest_wider("value") |>
  #unnest("creators") |>
  unnest("journalArticle")

zot$addto_collection(collection = "IZ58FGBA",  payload = my_refs[[1]])


```




```{r}

ids <- pluck(sss, "value...11")
ids[1]

my_refs <- zotero_references(ids)

zoteror::zot_add_to_collection(ids[[1]][[1]])

my_refs[[1]]$journalArticle |>
  glimpse()


```

